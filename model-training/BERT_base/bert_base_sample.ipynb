{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Note: This is a sample training and results for BERT Base - please see full results at result_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "import re\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import copy\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader - base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv('project_dtrain.csv', index_col = 0)\n",
    "dval = pd.read_csv('project_dval.csv', index_col = 0)\n",
    "dtest = pd.read_csv('project_dtest.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "#Check if CUDA is available\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - PRETRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "#Check if CUDA is available\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "from transformers.optimization import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv('project_dtrain.csv', index_col = 0)\n",
    "dval = pd.read_csv('project_dval.csv', index_col = 0)\n",
    "dtest = pd.read_csv('project_dtest.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "def bert_load(data):\n",
    "    '''\n",
    "    Load in data\n",
    "    Return BERT's preprocessed inputs including token_id, mask, label\n",
    "    '''\n",
    "    token_ids = []\n",
    "    attention_masks = []\n",
    "    for row in data['TEXT']:\n",
    "        row = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', row)\n",
    "        encoded_dict = tokenizer.encode_plus(row,\n",
    "                                            add_special_tokens= True, #add [CLS], [SEP]\n",
    "                                            max_length = 512,  \n",
    "                                            pad_to_max_length = True, #pad and truncate\n",
    "                                            return_attention_mask = True, #construct attention mask\n",
    "                                            return_tensors = 'pt') #return pytorch tensor\n",
    "        \n",
    "        token_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    token_ids = torch.cat(token_ids,dim=0)\n",
    "    attention_masks = torch.cat(attention_masks,dim=0)\n",
    "    labels = torch.tensor(data['READMIT'].values)\n",
    "    data_out = TensorDataset(token_ids, attention_masks, labels)\n",
    "    return data_out\n",
    "        \n",
    "datatrain = bert_load(dtrain)   \n",
    "dataval = bert_load(dval)\n",
    "datatest = bert_load(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "train_loaderB = DataLoader(datatrain,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True)\n",
    "                           \n",
    "\n",
    "val_loaderB = DataLoader(dataval,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle= True)\n",
    "                         \n",
    "\n",
    "test_loaderB = DataLoader(datatest,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2020)\n",
    "def trainBERT(model, train_loader, val_loader, num_epoch=20):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps= 1e-8) \n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    auc = []\n",
    "    best_auc = 0.\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        #Initialize\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (data, mask, labels) in enumerate(train_loader):\n",
    "            data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss, outputs = model(data, token_type_ids = None,\n",
    "                                  attention_mask= mask,\n",
    "                                  labels =labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "            pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "            total += labels.size(0)\n",
    "            correct += float(sum((pred ==label_cpu)))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc = correct/total\n",
    "        t_loss = total_loss/total\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(acc)\n",
    "        # report performance\n",
    "        \n",
    "        print('Epoch: ',epoch)\n",
    "        print('Train set | Accuracy: {:6.4f} | Loss: {:6.4f}'.format(acc, t_loss))     \n",
    "    \n",
    "    # Evaluate after every epoch\n",
    "        #Reset the initialization\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        model.eval()\n",
    "        \n",
    "        predictions =[]\n",
    "        truths= []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, mask, labels) in enumerate(val_loader):\n",
    "                data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "                model.zero_grad()\n",
    "\n",
    "                va_loss, outputs = model(data, token_type_ids = None,\n",
    "                                      attention_mask= mask,\n",
    "                                      labels =labels)\n",
    "\n",
    "                label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "                \n",
    "                pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "                total += labels.size(0)\n",
    "                correct += float(sum((pred ==label_cpu)))\n",
    "                total_loss += va_loss.item()\n",
    "                \n",
    "                predictions += list(pred)\n",
    "                truths += list(label_cpu)\n",
    "                       \n",
    "            v_acc = correct/total\n",
    "            v_loss = total_loss/total\n",
    "            val_loss.append(v_loss)\n",
    "            val_acc.append(v_acc)\n",
    "            \n",
    "            v_auc = roc_auc_score(truths, predictions)\n",
    "            auc.append(v_auc)\n",
    "            \n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Validation set | Accuracy: {:6.4f} | AUC: {:6.4f} | Loss: {:4.2f} | time elapse: {:>9}'.format(\n",
    "                v_acc, v_auc, v_loss, elapse))\n",
    "            print('-'*10)\n",
    "            \n",
    "            if v_auc > best_auc:\n",
    "                best_auc = v_auc\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best validation auc: {:6.4f}'.format(best_auc))\n",
    "    model.load_state_dict(best_model)     \n",
    "    return train_loss, train_acc, val_loss, val_acc, v_auc, model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modelBERT\n",
    "# torch._C._cuda_emptyCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empty cache\n",
    "#torch.cuda.empty_cache()\n",
    "modelBERT = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "modelBERT.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRETRAINED BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train set | Accuracy: 0.7830 | Loss: 0.0393\n",
      "Validation set | Accuracy: 0.7316 | AUC: 0.5834 | Loss: 0.04 | time elapse:  00:03:14\n",
      "----------\n",
      "Epoch:  1\n",
      "Train set | Accuracy: 0.8137 | Loss: 0.0351\n",
      "Validation set | Accuracy: 0.7679 | AUC: 0.6232 | Loss: 0.04 | time elapse:  00:06:29\n",
      "----------\n",
      "Best validation accuracy: 0.7679\n"
     ]
    }
   ],
   "source": [
    "#Train BERT with batch size 12 for 2 epochs\n",
    "train_loss_BERT, train_acc_BERT, val_loss_BERT, val_acc_BERT, val_auc_BERT, model_BERT = trainBERT(modelBERT, \n",
    "                                                                                     train_loaderB, \n",
    "                                                                                     val_loaderB, \n",
    "                                                                                     num_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train set | Accuracy: 0.7754 | Loss: 0.0446\n",
      "Validation set | Accuracy: 0.7766 | AUC: 0.5000 | Loss: 0.04 | time elapse:  00:05:33\n",
      "----------\n",
      "Epoch:  1\n",
      "Train set | Accuracy: 0.7761 | Loss: 0.0413\n",
      "Validation set | Accuracy: 0.7495 | AUC: 0.5889 | Loss: 0.04 | time elapse:  00:11:07\n",
      "----------\n",
      "Epoch:  2\n",
      "Train set | Accuracy: 0.8034 | Loss: 0.0363\n",
      "Validation set | Accuracy: 0.7088 | AUC: 0.6241 | Loss: 0.04 | time elapse:  00:16:40\n",
      "----------\n",
      "Epoch:  3\n",
      "Train set | Accuracy: 0.8535 | Loss: 0.0296\n",
      "Validation set | Accuracy: 0.7625 | AUC: 0.6154 | Loss: 0.04 | time elapse:  00:22:13\n",
      "----------\n",
      "Epoch:  4\n",
      "Train set | Accuracy: 0.9139 | Loss: 0.0195\n",
      "Validation set | Accuracy: 0.7749 | AUC: 0.6269 | Loss: 0.06 | time elapse:  00:27:46\n",
      "----------\n",
      "Epoch:  5\n",
      "Train set | Accuracy: 0.9461 | Loss: 0.0123\n",
      "Validation set | Accuracy: 0.7858 | AUC: 0.6218 | Loss: 0.06 | time elapse:  00:33:20\n",
      "----------\n",
      "Best validation auc: 0.6269\n"
     ]
    }
   ],
   "source": [
    "#Train BERT with batch size 12 for 6 epochs\n",
    "train_loss_BERT2, train_acc_BERT2, val_loss_BERT2, val_acc_BERT2, val_auc_BERT2, model_BERT2 = trainBERT(modelBERT, \n",
    "                                                                                     train_loaderB, \n",
    "                                                                                     val_loaderB, \n",
    "                                                                                     num_epoch=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch = np.arange(0,6,1)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(epoch, train_loss_BERT2, label='train loss')\n",
    "plt.plot(epoch, val_loss_BERT2, label='validation loss')\n",
    "plt.title('Plot of train and validation loss per epoch',fontsize = 15)\n",
    "plt.xlabel('epoch',fontsize = 15)\n",
    "plt.xticks(epoch)\n",
    "plt.ylabel('loss',fontsize = 15)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(epoch, train_acc_BERT2, label='train accuracy')\n",
    "plt.plot(epoch, val_acc_BERT2, label='validation accuracy')\n",
    "plt.title('Plot of train and validation accuracy per epoch',fontsize=15)\n",
    "plt.xlabel('epoch',fontsize = 15)\n",
    "plt.ylabel('accuracy rate',fontsize = 15)\n",
    "plt.xticks(epoch)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(epoch, train_auc_BERT2, label='Best AUC: 0.6269')\n",
    "plt.title('Plot of train and validation AUC per epoch',fontsize=15)\n",
    "plt.xlabel('epoch',fontsize = 15)\n",
    "plt.ylabel('auc',fontsize = 15)\n",
    "plt.xticks(epoch)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_BERT2,'model_bert_pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Accuracy: 0.8021\n",
      "Test set | AUC: 0.6530\n"
     ]
    }
   ],
   "source": [
    "#TEST SET\n",
    "torch.manual_seed(2020)\n",
    "model = model_BERT2.to(device)\n",
    "model.eval()\n",
    "total =0.\n",
    "correct = 0.\n",
    "predictions =[]\n",
    "truths= []\n",
    "\n",
    "with torch.no_grad():        \n",
    "    for i, (data, mask, labels) in enumerate(test_loaderB):\n",
    "        data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        _, outputs = model(data, token_type_ids = None,\n",
    "                                      attention_mask= mask,\n",
    "                                      labels =labels)\n",
    "\n",
    "        label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "        pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "        total += labels.size(0)\n",
    "        correct += float(sum((pred ==label_cpu))) \n",
    "        \n",
    "        predictions += list(pred)\n",
    "        truths += list(label_cpu)\n",
    "        \n",
    "    v_auc = roc_auc_score(truths, predictions)\n",
    "    v_acc = correct/total\n",
    "    \n",
    "    \n",
    "print('Test set | Accuracy: {:6.4f}'.format(v_acc))\n",
    "print('Test set | AUC: {:6.4f}'.format(v_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze all layers except the classifier -> train classifier\n",
    "torch.cuda.empty_cache()\n",
    "model = model_BERT2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRIAL 1- forget to mention classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train set | Accuracy: 0.9682 | Loss: 0.0088\n",
      "Validation set | Accuracy: 0.7652 | AUC: 0.6362 | Loss: 0.07 | time elapse:  00:02:24\n",
      "----------\n",
      "Epoch:  1\n",
      "Train set | Accuracy: 0.9640 | Loss: 0.0092\n",
      "Validation set | Accuracy: 0.7652 | AUC: 0.6362 | Loss: 0.07 | time elapse:  00:04:49\n",
      "----------\n",
      "Epoch:  2\n",
      "Train set | Accuracy: 0.9637 | Loss: 0.0091\n",
      "Validation set | Accuracy: 0.7652 | AUC: 0.6370 | Loss: 0.07 | time elapse:  00:07:14\n",
      "----------\n",
      "Epoch:  3\n",
      "Train set | Accuracy: 0.9640 | Loss: 0.0089\n",
      "Validation set | Accuracy: 0.7657 | AUC: 0.6356 | Loss: 0.07 | time elapse:  00:09:39\n",
      "----------\n",
      "Epoch:  4\n",
      "Train set | Accuracy: 0.9642 | Loss: 0.0091\n",
      "Validation set | Accuracy: 0.7652 | AUC: 0.6362 | Loss: 0.07 | time elapse:  00:12:04\n",
      "----------\n",
      "Best validation auc: 0.6370\n"
     ]
    }
   ],
   "source": [
    "train_loss_BERT_fc, train_acc_BERT_fc, val_loss_BERT_fc, val_acc_BERT_fc,val_auc_BERT_fc, model_BERT2_fc = trainBERT(model, \n",
    "                                                                                     train_loaderB, \n",
    "                                                                                     val_loaderB, \n",
    "                                                                                     num_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Accuracy: 0.7923\n",
      "Test set | AUC: 0.6683\n"
     ]
    }
   ],
   "source": [
    "#TEST SET\n",
    "torch.manual_seed(2020)\n",
    "model =  model_BERT2_fc.to(device)\n",
    "model.eval()\n",
    "total =0.\n",
    "correct = 0.\n",
    "predictions =[]\n",
    "truths= []\n",
    "\n",
    "with torch.no_grad():        \n",
    "    for i, (data, mask, labels) in enumerate(test_loaderB):\n",
    "        data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        _, outputs = model(data, token_type_ids = None,\n",
    "                                      attention_mask= mask,\n",
    "                                      labels =labels)\n",
    "\n",
    "        label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "        pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "        total += labels.size(0)\n",
    "        correct += float(sum((pred ==label_cpu))) \n",
    "        \n",
    "        predictions += list(pred)\n",
    "        truths += list(label_cpu)\n",
    "        \n",
    "    v_auc = roc_auc_score(truths, predictions)\n",
    "    v_acc = correct/total\n",
    "    \n",
    "    \n",
    "print('Test set | Accuracy: {:6.4f}'.format(v_acc))\n",
    "print('Test set | AUC: {:6.4f}'.format(v_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRIAL 2-mmention classifier + increase learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(2020)\n",
    "def trainBERT_classifier(model, train_loader, val_loader, num_epoch=20, lr = 0.001):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, eps= 1e-8) \n",
    "    \n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    auc = []\n",
    "    best_auc = 0.\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        #Initialize\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (data, mask, labels) in enumerate(train_loader):\n",
    "            data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss, outputs = model(data, token_type_ids = None,\n",
    "                                  attention_mask= mask,\n",
    "                                  labels =labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "            pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "            total += labels.size(0)\n",
    "            correct += float(sum((pred ==label_cpu)))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc = correct/total\n",
    "        t_loss = total_loss/total\n",
    "        train_loss.append(t_loss)\n",
    "        train_acc.append(acc)\n",
    "        # report performance\n",
    "        \n",
    "        print('Epoch: ',epoch)\n",
    "        print('Train set | Accuracy: {:6.4f} | Loss: {:6.4f}'.format(acc, t_loss))     \n",
    "    \n",
    "    # Evaluate after every epoch\n",
    "        #Reset the initialization\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        model.eval()\n",
    "        \n",
    "        predictions =[]\n",
    "        truths= []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, mask, labels) in enumerate(val_loader):\n",
    "                data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "                model.zero_grad()\n",
    "\n",
    "                va_loss, outputs = model(data, token_type_ids = None,\n",
    "                                      attention_mask= mask,\n",
    "                                      labels =labels)\n",
    "\n",
    "                label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "                \n",
    "                pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "                total += labels.size(0)\n",
    "                correct += float(sum((pred ==label_cpu)))\n",
    "                total_loss += va_loss.item()\n",
    "                \n",
    "                predictions += list(pred)\n",
    "                truths += list(label_cpu)\n",
    "                       \n",
    "            v_acc = correct/total\n",
    "            v_loss = total_loss/total\n",
    "            val_loss.append(v_loss)\n",
    "            val_acc.append(v_acc)\n",
    "            \n",
    "            v_auc = roc_auc_score(truths, predictions)\n",
    "            auc.append(v_auc)\n",
    "            \n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Validation set | Accuracy: {:6.4f} | AUC: {:6.4f} | Loss: {:4.2f} | time elapse: {:>9}'.format(\n",
    "                v_acc, v_auc, v_loss, elapse))\n",
    "            print('-'*10)\n",
    "            \n",
    "            if v_auc > best_auc:\n",
    "                best_auc = v_auc\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Best validation auc: {:6.4f}'.format(best_auc))\n",
    "    model.load_state_dict(best_model)     \n",
    "    return train_loss, train_acc, val_loss, val_acc, v_auc, model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train set | Accuracy: 0.9665 | Loss: 0.0091\n",
      "Validation set | Accuracy: 0.7684 | AUC: 0.6348 | Loss: 0.06 | time elapse:  00:02:24\n",
      "----------\n",
      "Epoch:  1\n",
      "Train set | Accuracy: 0.9653 | Loss: 0.0087\n",
      "Validation set | Accuracy: 0.7608 | AUC: 0.6472 | Loss: 0.08 | time elapse:  00:04:50\n",
      "----------\n",
      "Epoch:  2\n",
      "Train set | Accuracy: 0.9684 | Loss: 0.0086\n",
      "Validation set | Accuracy: 0.7739 | AUC: 0.6340 | Loss: 0.09 | time elapse:  00:07:15\n",
      "----------\n",
      "Epoch:  3\n",
      "Train set | Accuracy: 0.9647 | Loss: 0.0087\n",
      "Validation set | Accuracy: 0.7749 | AUC: 0.6407 | Loss: 0.07 | time elapse:  00:09:40\n",
      "----------\n",
      "Epoch:  4\n",
      "Train set | Accuracy: 0.9656 | Loss: 0.0092\n",
      "Validation set | Accuracy: 0.7603 | AUC: 0.6460 | Loss: 0.08 | time elapse:  00:12:06\n",
      "----------\n",
      "Best validation auc: 0.6472\n"
     ]
    }
   ],
   "source": [
    "train_loss_BERT_fc2, train_acc_BERT_fc2, val_loss_BERT_fc2, val_acc_BERT_fc2,val_auc_BERT_fc2, model_BERT2_fc2 = trainBERT_classifier(model, \n",
    "                                                                                     train_loaderB, \n",
    "                                                                                     val_loaderB, \n",
    "                                                                                     num_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set | Accuracy: 0.7852\n",
      "Test set | AUC: 0.6724\n"
     ]
    }
   ],
   "source": [
    "#TEST SET\n",
    "torch.manual_seed(2020)\n",
    "model =  model_BERT2_fc2.to(device)\n",
    "model.eval()\n",
    "total =0.\n",
    "correct = 0.\n",
    "predictions =[]\n",
    "truths= []\n",
    "\n",
    "with torch.no_grad():        \n",
    "    for i, (data, mask, labels) in enumerate(test_loaderB):\n",
    "        data, mask, labels = data.to(device), mask.to(device), labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        _, outputs = model(data, token_type_ids = None,\n",
    "                                      attention_mask= mask,\n",
    "                                      labels =labels)\n",
    "\n",
    "        label_cpu = labels.squeeze().to('cpu').numpy()\n",
    "        pred = outputs.data.max(-1)[1].to('cpu').numpy()\n",
    "        total += labels.size(0)\n",
    "        correct += float(sum((pred ==label_cpu))) \n",
    "        \n",
    "        predictions += list(pred)\n",
    "        truths += list(label_cpu)\n",
    "        \n",
    "    v_auc = roc_auc_score(truths, predictions)\n",
    "    v_acc = correct/total\n",
    "    \n",
    "    \n",
    "print('Test set | Accuracy: {:6.4f}'.format(v_acc))\n",
    "print('Test set | AUC: {:6.4f}'.format(v_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_BERT2_fc2,'model_bert_pretrained_fc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6459870098172155"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch = np.arange(0,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change learning rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  model_BERT2.to(device)\n",
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train set | Accuracy: 0.9664 | Loss: 0.0086\n",
      "Validation set | Accuracy: 0.7706 | AUC: 0.6397 | Loss: 0.07 | time elapse:  00:02:25\n",
      "----------\n",
      "Epoch:  1\n",
      "Train set | Accuracy: 0.9669 | Loss: 0.0085\n",
      "Validation set | Accuracy: 0.7674 | AUC: 0.6402 | Loss: 0.07 | time elapse:  00:04:50\n",
      "----------\n",
      "Epoch:  2\n",
      "Train set | Accuracy: 0.9656 | Loss: 0.0085\n",
      "Validation set | Accuracy: 0.7684 | AUC: 0.6400 | Loss: 0.07 | time elapse:  00:07:16\n",
      "----------\n",
      "Epoch:  3\n",
      "Train set | Accuracy: 0.9684 | Loss: 0.0080\n",
      "Validation set | Accuracy: 0.7684 | AUC: 0.6409 | Loss: 0.07 | time elapse:  00:09:41\n",
      "----------\n",
      "Epoch:  4\n",
      "Train set | Accuracy: 0.9660 | Loss: 0.0082\n",
      "Validation set | Accuracy: 0.7711 | AUC: 0.6400 | Loss: 0.07 | time elapse:  00:12:07\n",
      "----------\n",
      "Best validation auc: 0.6409\n"
     ]
    }
   ],
   "source": [
    "train_loss_BERT_fc3, train_acc_BERT_fc3, val_loss_BERT_fc3, val_acc_BERT_fc3,val_auc_BERT_fc3, model_BERT2_fc3 = trainBERT_classifier(model, \n",
    "                                                                                     train_loaderB, \n",
    "                                                                                     val_loaderB, \n",
    "                                                                                     num_epoch=5,\n",
    "                                                                                    lr = 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
